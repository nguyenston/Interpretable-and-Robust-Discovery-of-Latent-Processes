% !TEX root =  presentation_arxiv.tex
%
% table of content
\section{Introduction}
\begin{frame}{Introduction}
  \only<1-2>{\pdfnote{
      Many important real-world applications in healthcare, autonomous systems and scientific discovery are fundamentally
      driven by unobserved, latent proceses.
      For example, these processes can be <walk through examples>.
  }}
  \only<3>{
    \pdfnote{
      Problem is, many methods developed for these applications, while excel at prediction, is not as good
      at reliably identify these underlying latent structures and in a way that makes human sense.
      A large portion of them rely on local search (like Expectation-Maximization) and is therefore prone to local convergence
      Most of them require the number of processes as a hyperparameter, common approaches such as BIC has been proven
      to not fair well against real-life complications like sparse, incomplete, and noisy data, or model misspecification
    }
  }
  Type of latent processes in real-world applicatons:
  \pause
  \begin{itemize}
    \item Mutational signatures (tumor mutational signature analysis) 
      \citep{Levitin_DeNovo_Gene_Signature_Identification_2019,Kinker_Pan_Cancer_2020,Seplyarskiy_PopulationSequencingData_2021}
    \item Material types (remote sensing)
      \citep{Fevotte_NonlinearHyperspectralUnmixing_2015,Rajabi_SpectralUnmixingHyperspectral_2015}
    \item Decision policies (HRL/HIL)
      \citep{SUTTON1999181, Barto2003}
  \end{itemize}
  \pause
  \begin{alertblock}{Problem statement}
    Current machine learning techniques excel at prediction, but bad at
    consistent and interpretable identification of these underlying latent structures that govern
    observable phenomena:
    \begin{itemize}
      \item Rely on local search $\implies$ local convergence, sensitivity to initialization
      \item Require the number of processes apriori $\implies$ commonly used techniques weak to complications
    \end{itemize}
  \end{alertblock}
\end{frame}

\begin{frame}{Outline of contribution}
  \pdfnote{
    With that in mind, this prospectus will cover two methods that aim to address these challenges
    <go through methods>. Method of moments for HIL, asymptotically consistent, offers global convergence under mid
    assumptions and with only one pass over the data. Accumulated Cutoff Discrepancy Criterion (ACDC) <joke name Huggins>
    model selection framework (it tells you how many processes there should be) that is applicable to a variety of problems,
    even when the model doesn't match perfectly with the true data generating process. I will be walking through the two methods
    and present the experiments, then we will talk about future directions.
  }
  \tableofcontents
\end{frame}

\section{Method of moments for HIL}

\subsection{Hierarchical Reinforcement Learning}
  \pdfnote{
    HRL is an approach that seeks to make RL more scalable and interpretable.
    <walk through flow chart, explain the high and low level><walk through example>
  }
\begin{frame}{Hierarchical Reinforcement Learning}
    \begin{columns}[b]
        \column{0.4\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{figures/layers_of_policies.pdf}
                \caption{Multiple layers of policies}
                \label{fig:policy_layers}
            \end{figure}
        \column{0.6\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{figures/monezuma_revenge_atari_game.pdf}
                \caption{Monezuma's Revenge}
                \label{fig:4_room_grid_world}
            \end{figure}
    \end{columns}
\end{frame}

\subsection{HIL models and frameworks}
\begin{frame}{Options framework and Hierarchical Imitation Learning}
    \pdfnote{
      In cases where expert demonstration is available, the learning process can be accelerated by imitating the expert first,
      this is called HIL. This method will be focusing on learning the following model called the options framework.
      <explains the bayesian graph>
    }

    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth, page=1]{figures/options_bayesian_net.pdf}
        \caption{Bayesian graph of the options framework}
        \label{fig:options_chain}
    \end{figure}
\end{frame}

\begin{frame}{Hidden Markov Model}
    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth, page=1]{figures/hmm_bayesian_net.pdf}
        \caption{Bayesian graph of HMM}
        \label{fig:HM_chain}
    \end{figure}
    \pause
    Existing method for HMM (namely the EM based Baum-Welch algorithm) has been adapted to work with the options framework \citep{zhiyu20,Giammarino_2021}.
    \begin{alertblock}{}
        What if we also adapt another class of methods?
    \end{alertblock}
\end{frame}

\subsection{Method of moments}
\begin{frame}{Method of moments}
    Challenges of EM based methods:
    \begin{itemize}
        \item Local convergence
        \item Iterative method with expensive steps
        \item Performance depends on initialization
    \end{itemize}
    \begin{block}{}
        Method of moments are able to address each of these problems.
    \end{block}
\end{frame}

\begin{frame}{Method of moments}
  \center{\LARGE{What are moments?}}
    \begin{columns}[b]
        \column{0.563\textwidth}
          \begin{figure}
              \centering
              \includegraphics[width=\textwidth, page=2]{figures/options_bayesian_net.pdf}
              \caption{Bayesian graph of the options framework}
          \end{figure}
        \column{0.437\textwidth}
          \begin{figure}
              \centering
              \includegraphics[width=\textwidth, page=2]{figures/hmm_bayesian_net.pdf}
              \caption{Bayesian graph of HMM}
          \end{figure}
    \end{columns}
\end{frame}

\begin{frame}{Method of moments for HMM}
    \begin{Definition}
      \begin{enumerate}
          \item Hidden parameters: 
          \begin{enumerate}
              \item $\bm{P}[i,j] = P\lrp{X_{t+1}=j\vert X_t=i}$
              \item $\bm{Q}[i,j] = P\lrp{Y_t=j\vert X_t=i}$
          \end{enumerate}
          \item Observable moments:
          \begin{enumerate}
              \item $\bm{M}_y[i,j] = P\lrp{Y_1=i,Y_2=y,Y_3=j}$
              \item $\bm{M}[i,j] = P\lrp{Y_1=i,Y_3=j}$
          \end{enumerate}
      \end{enumerate}
    \end{Definition}
    \pause
    \begin{Theorem}
      The following product admits the factorization \citep{hsu08}:
        \[
            \bm{M}_y\inv\bm{M} = 
            (\bm{P}\bm{Q})\inv\mathrm{diag}(\bm{Q}\unit_y)\bm{P}\bm{Q}
        \]
    \end{Theorem}
    \pause
    \begin{alertblock}{Note}
        This equation is the main concept behind the algorithm. Finding $\bm{P}$ after having found $\bm{Q}$ is straight forward.
    \end{alertblock}
\end{frame}

\begin{frame}{Method of moments for HIL}
    \begin{figure}
        \centering
        \includegraphics[width=0.75\textwidth, page=2]{figures/options_bayesian_net.pdf}
    \end{figure}
    \begin{block}{Formulation}
        The triple $(O_t,S_t,A_t)$ forms a discrete-time Markov chain with $O_t$, $S_t$, and $A_t$ defined over the finite spaces $\mathcal{O}$, $\mathcal{S}$, and $\mathcal{A}$, respectively.
        We denote the cardinality of these spaces as 
        $\vert\mathcal{O}\vert=\omega$, $\vert\mathcal{S}\vert=\zeta$, and $\vert\mathcal{A}\vert=\alpha$.
    \end{block}
\end{frame}

\begin{frame}{Method of moments for HIL - Defining parameters}

    \begin{Definition}
        \begin{flalign}
        \text{Let}
        &&\bm{\Pi}^{lo}_s[o,a]&= \pi_{lo}(A_t=a\vert O_t=o,S_t=s),&&\\
        &&\bm{\Pi}^{hi}_s[o,o']&= \pi_{hi}(O_{t+1} = o'\vert O_t = o,S_{t+1}=s),&&\\
        &&\bm{\Phi}^A_a[s,s']&=\Phi(S_{t+1} = s'\vert S_t = s, A_t=a).&&
      \end{flalign}
    \end{Definition}
    \pause
    \begin{block}{Block diagonal forms}
        \begin{flalign}
          \text{Let\qquad\qquad\qquad\quad}&&
          \bm{\Pi}^{lo}\lrb{s\zeta+o,s'\zeta+a} &= \pi_{lo}(A_1=a,S_1=s'\vert O_1=o,S_1=s),&&\\
          &&\bm{\Pi}^{hi}\lrb{s\zeta+o,s'\zeta+o'} &= \pi_{hi}(O_2=o',S_1=s'\vert O_1=o,S_2=s).&&
        \end{flalign}
        \begin{flalign}
          \text{These have the form: }&&
          \bm{\Pi}^{lo}=\bmat{\bm{\Pi}^{lo}_1&&\\&\ddots&\\&&\bm{\Pi}^{lo}_\zeta}
          \text{ and }
          \bm{\Pi}^{hi}=\bmat{\bm{\Pi}^{hi}_1&&\\&\ddots&\\&&\bm{\Pi}^{hi}_\zeta}&&.
        \end{flalign}
    \end{block}
\end{frame}

\begin{frame}{Method of moments for HIL - Defining moments}
        
    \begin{Definition}
        \begin{flalign}
          \text{Let}
          &&\bm{M}_a\lrb{s_2\zeta+s_1,s_3\alpha+a_3}
          &=P(S_2=s_2,S_1=s_1,A_2=a,S_3=s_3,A_3=a_3),&&\\
          &&\bm{K}_s[s_1,a_2]&=P(S_1=s_1,S_2=s,A_2=a_2).&&
      \end{flalign}
    \end{Definition}
    \pause
    \begin{block}{Goal}
        Construct a diagonalizable expression out of these moments.
    \end{block}
    \pause
    \begin{alertblock}{Problems}
      \begin{itemize}
        \item Transition dynamics $\bm{\Phi}^A_a$ introduces complications
        \item  $\alpha\neq\omega\implies\bm{\Pi}^{lo}$ is non-square $\implies$ can't invert
      \end{itemize}
    \end{alertblock}
\end{frame}

% \begin{frame}{Method of moments for HIL - Additional definitions}
%     Let
%         \[
%           \bm{\Xi}_{s'}[s,o] &= P(S_t=s,O_t=o,S_{t+1}=s'),\\
%           \bm{\Xi}[s'\zeta+s,s''\zeta+o] &= P(S_t=s,O_t=o,S_{t+1}=s'=s'').
%         \]
%         We can express $\bm{\Xi}$ as:
%         \[\bm{\Xi}=\bmat{\bm{\Xi}_1&&\\&\ddots&\\&&\bm{\Xi}_\zeta}.\]
%     \begin{alertblock}{Note}
%         Only necessary for proof and theory. Should not be relevant during implementation.
%     \end{alertblock}
% \end{frame}
%
% \begin{frame}{Method of moments for HIL - Assumptions}
%     \begin{block}{Rank assumptions}
%         \begin{enumerate}
%             \item $\bm{\Pi}^{lo}$ has full row rank
%             \item $\bm{\Pi}^{hi}$ has full rank
%             \item $\bm{\Xi}$ has full column rank
%         \end{enumerate}
%     \end{block}
%     \pause
%     \begin{block}{Noisy transition assumption}
%         For any $s,s'\in\mathcal{S}$, if there exists $a\in\mathcal{A}$ such that $\bm{\Phi}^A_a[s,s']>0$, then $\bm{\Phi}^A_{a'}[s,s']>0,\forall a'\in\mathcal{A}.$
%     \end{block}
%     \pause
%     \begin{block}{Stationarity assumption}
%         The process $(O_t,S_t)$ starts with the stationary distribution, that is $\bm{\pi}^1_s[o]=\bm{\pi}^\infty_s[o]$ where $\bm{\pi}^t_s\in\mathbb{R}^\omega$ with $\bm{\pi}^t_s[o]=P(O_t=o,s_t=s)$ for $s \in \mathcal{S}$.
%     \end{block}
% \end{frame}



\begin{frame}{Method of moments for HIL - Projection lemma}
    Let $\bm{V}_s\in\mathbb{R}^{\alpha\times\omega}$ for $s \in \mathcal{S}$ be a matrix of right singular vectors corresponding to the $\omega$ largest singular values of $\bm{K}_s$, and define
    \[
        \bm{V} = \bmat{\bm{V}_1&&\\&\ddots&\\&&\bm{V}_\zeta}.
    \]
    \pause
    \begin{block}{Projection lemma}
        The product matrix
        \[
            \bm{\Pi}^{lo}\bm{V} = \bmat{\bm{\Pi}^{lo}_1\bm{V}_1&&\\&\ddots&\\&&\bm{\Pi}^{lo}_\zeta\bm{V}_\zeta}
        \]
        is invertible.
    \end{block}
\end{frame}

\begin{frame}{Spectral method of moments for HIL - Surrogate moments}
    Let the normalizer matrices $\bm{N}_a \in \mathbb{R}^{\zeta \times \zeta}$ 
    for $a \in \mathcal{A}$ 
    and kernel matrix $\bm{\Psi} \in \mathbb{R}^{\zeta \times \zeta}$ with
    \[
        \bm{N}_a[s_2,s_3]&=\left\{\begin{aligned}
            &\frac{1}{\bm{\Phi}^A_a[s_2,s_3]},~&\text{if}\ \bm{\Phi}^A_a[s_2,s_3]>0,\\
            &0,\quad&\text{otherwise,}
        \end{aligned}\right.
        \quad
        \bm{\Psi}[s_2,s_3]&=\left\{\begin{aligned}
            &\psi_{s_2s_3},~&\text{if}\ \bm{\Phi}^A_a[s_2,s_3]>0\ \forall a\in\mathcal{A},\\
            &0,\quad&\text{otherwise},
        \end{aligned}\right.
    \]
    where $\psi_{s_2s_3}$ are constants of choice such that $\bm{\Psi}$ is full rank.
    \pause
    \begin{block}{Surrogate moments}
        We can remove the influence of $\bm{\Phi}^A_a$ by working with the following surrogates instead:
        \[
            \bm{\hat{M}}_a = (\bm{\Psi}\otimes\one_{\zeta\times\alpha})\circ (\bm{N}_a\otimes\one_{\zeta\times\alpha})\circ \bm{M}_a,
        \]
        for $a \in \mathcal{A}$ and
        \[
          \bm{\hat{M}} = \sum_{a\in\mathcal{A}}\bm{\hat{M}}_a
        \]
    \end{block}
\end{frame}

\begin{frame}{Spectral method of moment for HIL - Main theorem}
    \begin{Theorem}
        The following product admits the factorization:
        \[\label{eq:diagonalize}
            \bm{V}\transpose\bm{\hat{M}}^+\bm{\hat{M}}_a\bm{V} = \bm{B}\inv\bm{\Lambda}_a\bm{B},
        \]
        where
        \[\label{eq:Lambda_definition}
            \bm{\Lambda}_a = \bmat{\mathrm{diag}(\bm{\Pi}^{lo}_1\unit_a)&&\\
                      &\ddots&\\
                      &&\mathrm{diag}(\bm{\Pi}^{lo}_{\zeta}\unit_a)}.
        \]
        \[
            \bm{B}=\lrp{\bm{\Psi}\otimes \identity_\omega}
            \bm{\Pi}^{hi}\bm{\Pi}^{lo}\bm{V}
        \]
    \end{Theorem}
    \pause
    \begin{alertblock}{Note on ordering}
        The eigen-decomposition will yield diagonals of the form
        $\bm{\mathcal{P}}\bm{\Lambda}_a\bm{\mathcal{P}}\transpose$.
        With some further processing, we can achieve $(\identity_\zeta\otimes\bm{\hat{\mathcal{P}}})\bm{\Lambda}_a(\identity_\zeta\otimes\bm{\hat{\mathcal{P}}}\transpose)$. This corresponds to the relabeling of the options.
    \end{alertblock}
\end{frame}

\begin{frame}{Spectral method of moments for HIL - Recover high-level policy}
    From the low level policies $\bm{\hat{\mathcal{P}}}\bm{\Pi}^{lo}_s$, the high level policies can be computed.
    \begin{block}{Theorem}
        \[
            \bm{\hat{\mathcal{P}}}\bm{\Pi}^{hi}_{s'}\bm{\hat{\mathcal{P}}}\transpose=
                \sum_s \bm{w}_{s'}[s]\lrp{\bm{\hat{\mathcal{P}}}\bm{\Pi}^{lo}_s
                \bm{K}_s^+\bm{\hat{K}}_{ss'}
                {\bm{\Pi}^{lo}_{s'}}^+\bm{\hat{\mathcal{P}}}\transpose},
        \]
        where:
        \begin{itemize}
            \item $\bm{\hat{K}}_{ss'}$ is a $\zeta\times\alpha$ submatrix of $\bm{\hat{M}}$ defined by
            \[\label{eq:hat_K_definition}
                \bm{\hat{K}}_{ss'}[s'',a]=\bm{\hat{M}}[s\zeta+s'',s'\alpha+a].
            \]
            \item $\bm{w}_{s'}$ are length $\zeta$ weight vectors of choice subject to
            \[\label{eq:pihi_weight_constraint}
                \bm{w}_i\transpose\bm{\Psi} \unit_i\transpose=1,\ \forall i\in\mathcal{S}.
            \]
        \end{itemize}
    \end{block}
\end{frame}

\subsection{Simulation}
\begin{frame}{Spectral method of moments for HIL - Simulation}
    Let there be a finite state machine with four states and the following parameters:
    \[
        \bm{\Pi}^{lo}_1=\bmat{0.6&0.4\\0.1&0.9},\quad
        \bm{\Pi}^{lo}_2=\bmat{0.7&0.3\\0.15&0.85},\\
        \bm{\Pi}^{lo}_3=\bmat{0.8&0.2\\0.3&0.7},\quad
        \bm{\Pi}^{lo}_4=\bmat{0.9&0.1\\0.35&0.65}.%\numberthis
    \]
    \[
        \bm{\Pi}^{hi}_1=\bmat{0.67&0.33\\0.16&0.84},\quad
        \bm{\Pi}^{hi}_2=\bmat{0.88&0.12\\0.16&0.84},\\
        \bm{\Pi}^{hi}_3=\bmat{0.84&0.16\\0.12&0.88},\quad
        \bm{\Pi}^{hi}_4=\bmat{0.84&0.16\\0.33&0.67}.%\numberthis
    \]
    \[
        \bm{\Phi}^A_1=\bmat{0.7&0.1&0.1&0.1\\
            0.4&0.4&0.1&0.1\\0.3&0.3&0.3&0.1\\
            0.25&0.25&0.25&0.25},\quad
        \bm{\Phi}^A_2=\bmat{0.25&0.25&0.25&0.25\\
            0.1&0.3&0.3&0.3\\0.1&0.1&0.4&0.4\\
            0.1&0.1&0.1&0.7}.%\numberthis
    \]
\end{frame}

\begin{frame}{Spectral method of moments for HIL - Simulation}
    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{figures/HIL_moments_error.pdf}
        \label{fig:error}
    \end{figure}
    The error is measured by:
    \[
        \mathrm{ERROR}=\sqrt{\norm{\bm{\Pi}^{lo}-\overline{\bm{\Pi}}^{lo}}^2_2
            +\norm{\bm{\Pi}^{hi}-\overline{\bm{\Pi}}^{hi}}^2_2},
    \]
    where $\overline{\bm{\Pi}}^{lo},\overline{\bm{\Pi}}^{hi}$ are the predicted values of $\bm{\Pi}^{lo},\bm{\Pi}^{hi}$.
\end{frame}

\begin{frame}{Spectral method of moments for HIL - Simulation}
    \begin{center}{$N=10^{5}$}\end{center}
    \begin{figure}
    \centering
        \includegraphics[width=0.5\textwidth]{figures/HIL_EM_error.pdf}
    \label{fig:EM_error}
\end{figure}
    The error is measured by:
    \[
        \mathrm{ERROR}=\sqrt{\norm{\bm{\Pi}^{lo}-\overline{\bm{\Pi}}^{lo}}^2_2
            +\norm{\bm{\Pi}^{hi}-\overline{\bm{\Pi}}^{hi}}^2_2},
    \]
    where $\overline{\bm{\Pi}}^{lo},\overline{\bm{\Pi}}^{hi}$ are the predicted values of $\bm{\Pi}^{lo},\bm{\Pi}^{hi}$.
\end{frame}

\section{Accumulated Cutoff Discrepancy Criterion}

\subsection{Challenges in model selection}
\begin{frame}{Determining the number of latent processes}
  Type of latent processes in real-world applicatons:
  \begin{itemize}
    \item Mutational signatures (tumor mutational signature analysis)
    \item Material types (remote sensing)
    \item decision policies (HRL/HIL)
  \end{itemize}
  \pause
  \begin{alertblock}{}
    $\implies$ Many current techniques requires the number of processes in advance
  \end{alertblock}
\end{frame}

\begin{frame}{Misspecification-induced overfitting}
  \begin{Examples}
    Mixture model of Gaussians, one of the components is skewwed. Using $N=10\,000$ observations.
  \end{Examples}
    \begin{columns}[b]
        \column{0.5\textwidth}
          \begin{figure}
              \centering
              \includegraphics[width=\textwidth]{figures/BIC_mixture.pdf}
              \caption{Overfitting}
          \end{figure}
        \column{0.5\textwidth}
          \begin{figure}
              \centering
              \includegraphics[width=\textwidth]{figures/ACDC_mixture.pdf}
              \caption{Desired behavior}
          \end{figure}
    \end{columns}
\end{frame}

\subsection{Component-level Discrepancy}
\begin{frame}{Component-level Discrepancy}
  \begin{figure}
      \centering
      \includegraphics[width=0.6\textwidth]{figures/ACDC_mixture_compwise_loss.pdf}
      % \caption{Component-wise loss}
  \end{figure}
  \pause
  \begin{block}{Robust model selection loss}
    \vspace{-1.0\baselineskip}
    \[ \label{eq:robust-loss}
      \mathcal{R}^\rho(x_{1:n}, K) = \sum_{k=1}^K \max(0, \compDiscEst^{(K,k)} - \rho),
    \]
  \end{block}
\end{frame}

\begin{frame}{Component-level Discrepancy}

\begin{flalign}
  \text{Mixture model class:}&&
  \textstyle \left\{ P_\theta = \sum_{k=1}^K \eta_k F_{\phi_k} : \theta = (\eta, \phi_1,\dots,\phi_K) \in \Theta^{(\numcomps)}\right\}.&&
  \phantom{\text{Mixture model cla}}
\end{flalign}
  \begin{algorithm}[H]
  \SetAlgoLined
  \KwIn{data $x_{1:N}$, maximum number of components allowed $K_{\max}$, and penalty parameter $\lambda$}
  \KwOut{Optimal estimated parameters $\hat{\theta}(\hat{K})$}
  \caption{ACDC for mixture model}
  \label{alg:model_selection}

  \For{$K = 1$ to $K_{\max}$}{
      Estimate model parameters $\hat{\theta}(K)$ based on $x_{1:N}$.
      Sample latent variables $z_{1:N}$ from the posterior distribution $p(z_{1:N} \mid \hat{\theta}(K); x_{1:N})$.
  }

  Determine an appropriate structural parameter $\rho$, if necessary.
  Compute the optimal number of components $\hat{K}$ by minimizing the penalized risk function:
    $
      \hat{K} = \arg \min_{K \in \{1, \dots, K_{\max}\}} R^{\lambda} (\hat{\theta}(K); x_{1:N}, z_{1:N}).
    $
  \Return $\hat{\theta}(\hat{K})$.

  \end{algorithm}
\end{frame}

\subsection{Modeling Framework}
\begin{frame}{Modeling Framework - Generalization of Mixture Modeling}
  \begin{columns}
      \column{0.4\textwidth}
      Consider the following generalized model
      \[
        \only<2->{z_n \mid \eta, w_n &\distind H^{(K)}_{\eta,w_n},\\}
        \only<1>{\vphantom{z_n \mid \eta, w_n \distind H^{(K)}_{\eta,w_n},}\\}
        \only<3->{\varepsilon_{nk} &\distiid G,\\}
        \only<1-2>{\vphantom{\varepsilon_{nk} \distiid G,}\\}
        \only<4->{y_{nk}&=f(w_{n}, z_{nk}, \phi_k, \varepsilon_{nk}) ,\\}
        \only<1-3>{\vphantom{y_{nk}=f(w_{n}, z_{nk}, \phi_k, \varepsilon_{nk}),}\\}
        \only<5->{x_n &= g^{(K)}(y_{n1}, \dots, y_{nK}).}
        \only<1-4>{\vphantom{x_n = g^{(K)}(y_{n1}, \dots, y_{nK}).}}
      \]
      \column{0.6\textwidth}
      \only<1>{
        \begin{figure}[tp]
          \centering
          \includegraphics[width=\textwidth,page=1]{model}
          % \caption{Graphical representation of the general form for model $\model{K}$ for a single observation $\data{n}$.
          %   Circles denote random variables while squares denote deterministic variables.
          %   A gray background indicates global parameters while a black background indicates an observed quantity.}
        \end{figure}
      }
      \only<2>{
        \begin{figure}[tp]
          \centering
          \includegraphics[width=\textwidth,page=2]{model}
        \end{figure}
      }
      \only<3>{
        \begin{figure}[tp]
          \centering
          \includegraphics[width=\textwidth,page=3]{model}
        \end{figure}
      }
      \only<4>{
        \begin{figure}[tp]
          \centering
          \includegraphics[width=\textwidth,page=4]{model}
        \end{figure}
      }
      \only<5>{
        \begin{figure}[tp]
          \centering
          \includegraphics[width=\textwidth,page=5]{model}
        \end{figure}
      }
  \end{columns}
\end{frame}

\begin{frame}{Modeling Framework - Examples}
  \begin{exampleblock}{Example 1: Mixture Modeling}
    We can recover a general mixture model by taking $H_\eta^{(\numcomps)} = \distCat(\eta)$, 
    so $z_{nk} \in \{0,1\}$ and $\sum_{k=1}^K z_{nk} = 1$.
    Given a mixture component distribution family $\mcF = \{ F_{\phi} \mid \phi \in \Phi \}$,
    define $f$ such that, for  $\varepsilon \distas G$, it holds that
      $f(0, \phi, \varepsilon) = 0$ and $f(1, \phi, \varepsilon) \distas F_\phi$. 
    Finally, take $g^{(K)}$ to be the summation operator.
    Hence, if $z_{nk} = 1$, then $x_n = y_{nk} \distas F_{\phi_k}$.
  \end{exampleblock}
  \pause
  \only<1,2>{\begin{exampleblock}{Example 2: Probabilistic Matrix Factorization}
    For probabilistic matrix factorization (PMF), $x_n \in \reals^{D}$.
    Let $\mcZ \subseteq \reals$ and $\Phi \subseteq \reals^{D}$.
    We assume that $\mcF = \{ F_{\mu} \mid \mu \in \reals^{D} \}$ is a location family 
    of distributions satisfying $\int x F_\mu(\dee x) = \mu$ for all $\mu \in \reals^{D}$.
    Let $f$ satisfy $f(z, \phi, \varepsilon) \distas F_{z \phi}$ if  $\varepsilon \distas G$.
    For example, in nonnegative matrix factorization, $F_{\mu} = \Poiss(\mu)$ 
    while in classical factor analysis $F_{\mu} = \Norm(\mu, \sigma^{2})$,
    where $\sigma^{2}$ can also be learned.
    Finally, take $g^{(K)}$ to be the summation operator.
  \end{exampleblock}}
  \only<3>{\begin{alertblock}{Example 2: Probabilistic Matrix Factorization 
      $\impliedby$ Simulation Focus}
    For probabilistic matrix factorization (PMF), $x_n \in \reals^{D}$.
    Let $\mcZ \subseteq \reals$ and $\Phi \subseteq \reals^{D}$.
    We assume that $\mcF = \{ F_{\mu} \mid \mu \in \reals^{D} \}$ is a location family 
    of distributions satisfying $\int x F_\mu(\dee x) = \mu$ for all $\mu \in \reals^{D}$.
    Let $f$ satisfy $f(z, \phi, \varepsilon) \distas F_{z \phi}$ if  $\varepsilon \distas G$.
    For example, in nonnegative matrix factorization, $F_{\mu} = \Poiss(\mu)$ 
    while in classical factor analysis $F_{\mu} = \Norm(\mu, \sigma^{2})$,
    where $\sigma^{2}$ can also be learned.
    Finally, take $g^{(K)}$ to be the summation operator.
  \end{alertblock}}
\end{frame}

\subsection{Generalized ACDC}
\begin{frame}{Generalized ACDC}
  \[
    z_n \mid \eta, w_n &\distind H^{(K)}_{\eta,w_n},\\
    \only<1>{\varepsilon_{nk} &\distiid G,\\}
    \only<2->{{\color{red}\varepsilon_{nk}} &\ {\color{red}\distiid G,}\\}
    y_{nk}&=f(w_{n}, z_{nk}, \phi_k, \varepsilon_{nk}) ,\\
    x_n &= g^{(K)}(y_{n1}, \dots, y_{nK}).
  \]
  \pause
  \pause
  \begin{block}{Reverse samling}
    \vspace{-1\baselineskip}
    \[
      \widehat G_{nk}^{(K)} = \mcL(\varepsilon_{nk} \mid x_n, w_n, \widehat\theta^{(K)}),\qquad
      \widehat G_k^{(K)} = \frac{1}{N} \sum_{n=1}^N \widehat G_{nk}^{(K)},
    \]
    \vspace{-1.5\baselineskip}
    \[
      \compDiscEst^{(K,k)} &= \discr{\widehat G_k^{(K)}}{G},\\
      \mathcal{R}^\rho(x_{1:n}, K) &= \sum_{k=1}^K \max(0, \compDiscEst^{(K,k)} - \rho).
    \]
  \end{block}
\end{frame}

\begin{frame}{Generalized ACDC - Choosing $\rho$}
  Ways to Choosing $\rho$:
  \begin{columns}[b]
      \column{0.45\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=0.9\textwidth]{figures/GvHD-train.pdf}
          \caption{Domain knowledge}
          % \caption{Selecting an optimal value of $\rho$ using the first 6 flow cytometry datasets.
          % 	The solid lines show $\rho$ vs.\ F-measure for datasets 1--6.
          % 	The black dashed line indicates averaged F-measure over the training datasets.
          % 	The vertical black line shows the value $\rho = 1.16$ that maximizes the averaged F-measure.}
          \label{fig:flowcyt-train}
        \end{figure}
      \column{0.55\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=0.9\textwidth]{figures/GvHD9-all-loss-plot-legend.pdf}
          \caption{Widely applicable heuristic}
        \end{figure}
  \end{columns}
\end{frame}

\subsection{Simulation}

\begin{frame}{Hyperspectral unmixing}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/urban_gt_5endmembers.pdf}
    \caption{Unlabeled (left) and labeled (right) versions of the urban dataset}
  \end{figure}
\end{frame}

\begin{frame}{Hyperspectral unmixing}
  \begin{columns}[t]
      \column{0.5\textwidth}
        \begin{figure}
          \centering
          \adjincludegraphics[width=\textwidth, Clip={0.00\width} {0.525\height} {0.00\width} {0.00\height}, clip]{figures/composite-cd-urban-l2h=m1e80-shuffle-simplexh-sig=rmse-by_1e4_LInf_sparsity.pdf}
          % \caption{Unlabeled (left) and labeled (right) versions of the urban dataset}
        \end{figure}
      \column{0.5\textwidth}
        \begin{figure}
          \centering
          \adjincludegraphics[width=\textwidth, Clip={0.00\width} {0.0\height} {0.00\width} {0.475\height}, clip]{figures/composite-cd-urban-l2h=m1e80-shuffle-simplexh-sig=rmse-by_1e4_LInf_sparsity.pdf}
          % \caption{Unlabeled (left) and labeled (right) versions of the urban dataset}
        \end{figure}
  \end{columns}
\end{frame}

\begin{frame}{Hyperspectral unmixing}
  \begin{columns}[t]
      \column{0.5\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{figures/urban_K=3.pdf}
          \caption{$K=3$}
        \end{figure}
      \column{0.5\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{figures/urban_K=4.pdf}
          \caption{$K=4$}
        \end{figure}
  \end{columns}
\end{frame}

\begin{frame}{Hyperspectral unmixing}
  \begin{columns}[t]
      \column{0.5\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{figures/urban_K=5.pdf}
          \caption{$K=5$}
        \end{figure}
      \column{0.5\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{figures/urban_K=6.pdf}
          \caption{$K=6$}
        \end{figure}
  \end{columns}
\end{frame}

\section{Discussions and Future Works}

\begin{frame}{Discussions and Future Works}
  Contributions:
  \begin{itemize}
    \item Method of moments for HIL: non-iterative algorithm for the options framework, 
      leveraging low-order observable moments to provides global and asymptotically consistent parameter estimation.
    \item ACDC: model selection method that introduces a ”cutoff discrepancy” at the component level, applicable to a wide variety of models and robust to model misspecification.
  \end{itemize}
  \pause
  Future directions:
  \begin{itemize}
    \item Relaxing rank assumptions
    \item More rigorous choice of $\rho$
    \pause
    \item ACDC is a very flexible framework:
    \begin{itemize}
      \item Apply to HMM for animal movement prediction
      \item Apply to Causal Representation Learning and Variational Auto Encoders
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{}

\end{frame}
